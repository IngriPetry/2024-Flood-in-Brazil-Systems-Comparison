{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import os\n",
    "import pickle\n",
    "# import cartopy.crs as ccrs\n",
    "import geopandas as gpd\n",
    "from datetime import timezone\n",
    "from dateutil import parser\n",
    "import matplotlib.colors as mcolors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_netcdf_summary(file_path, return_variables=None):\n",
    "\n",
    "    print(return_variables)\n",
    "    \"\"\"Prints a summary and optionally returns data for specific variables from a NetCDF file.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: The file '{file_path}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        dataset = nc.Dataset(file_path, mode='r')\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening file: {e}\")\n",
    "        return\n",
    "\n",
    "    variable_data = {}\n",
    "\n",
    "    print(\"Global Attributes:\")\n",
    "    for attr in dataset.ncattrs():\n",
    "        print(f\"  {attr}: {dataset.getncattr(attr)}\")\n",
    "\n",
    "    print(\"\\nDimensions:\")\n",
    "    for dim, dim_obj in dataset.dimensions.items():\n",
    "        print(f\"  {dim}: length {dim_obj.size} (unlimited: {dim_obj.isunlimited()})\")\n",
    "\n",
    "    print(\"\\nVariables:\")\n",
    "    for var in dataset.variables:\n",
    "        print(f\"Variable: {var}\")\n",
    "        print(f\"  Type: {dataset.variables[var].dtype}\")\n",
    "        print(f\"  Dimensions: {dataset.variables[var].dimensions}\")\n",
    "        print(f\"  Shape: {dataset.variables[var].shape}\")\n",
    "        for attr in dataset.variables[var].ncattrs():\n",
    "            print(f\"    {attr}: {dataset.variables[var].getncattr(attr)}\")\n",
    "        if return_variables and var in return_variables:\n",
    "            variable_data[var] = dataset.variables[var][:]\n",
    "\n",
    "    if 'time' in dataset.variables:\n",
    "        T_var = dataset.variables['time']\n",
    "        T_units = T_var.units if 'units' in T_var.ncattrs() else 'No units available'\n",
    "        T_data = T_var[:]\n",
    "        print(f\"\\nT units: {T_units} Data: {T_data}\")\n",
    "\n",
    "    dataset.close()\n",
    "\n",
    "    if return_variables:\n",
    "        print('retorna!')\n",
    "        return variable_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_station(folder_path, file_name):\n",
    "    \n",
    "    full_path = os.path.join(folder_path, f'{file_name}.txt')\n",
    "    try:\n",
    "        # Load observation data from a text file\n",
    "        # Specify -99999 and -1 as NaN values\n",
    "        df = pd.read_table(full_path, delim_whitespace=True, na_values=[-99999, -1])\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        bin_data = 0\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No data: {full_path}\")\n",
    "        bin_data = 1\n",
    "        df = pd.DataFrame()  # Return empty DataFrame if file is not found\n",
    "    \n",
    "    return df, bin_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def list_files_and_dates(directory_path):\n",
    "    \"\"\"Checks if the directory exists and returns a DataFrame containing filenames, their issue dates, and forecast datetimes.\"\"\"\n",
    "    if os.path.exists(directory_path):\n",
    "        # Initialize lists to hold file names, issue dates, and forecast datetimes\n",
    "        forecasts = []  # Initialize the list to store directory names\n",
    "        if os.path.exists(directory_path):\n",
    "            # List all entries in the given folder\n",
    "            for entry in os.listdir(directory_path):\n",
    "                # Construct the full path of the entry\n",
    "                full_path = os.path.join(directory_path, entry)\n",
    "                # Check if this entry is a directory\n",
    "                if os.path.isdir(full_path):\n",
    "                    forecasts.append(entry)  # Add the directory name to the list\n",
    "        else:\n",
    "            print(f\"The directory '{directory_path}' does not exist.\")\n",
    "\n",
    "\n",
    "        file_dates = [pd.to_datetime(dates, format='%Y%m%d%H%M') for dates in forecasts]\n",
    "\n",
    "\n",
    "        file_data = pd.DataFrame({\n",
    "            'File Name': forecasts,\n",
    "            'Date': file_dates\n",
    "        }).sort_values(by='Date').reset_index(drop=True)\n",
    "        return file_data\n",
    "    else:\n",
    "        print(f\"The directory '{directory_path}' does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nasa_rapid_MOGREPS(directory_path, file_data, river_id):\n",
    "    forecast_dict = {}  # Use a dictionary to store the data by issue_date\n",
    "\n",
    "    for forecast in file_data['File Name']:\n",
    "        issue_date = pd.to_datetime(forecast, format='%Y%m%d%H').replace(tzinfo=timezone.utc)\n",
    "        full_path = os.path.join(directory_path, forecast)\n",
    "\n",
    "        if not os.path.exists(full_path):\n",
    "            print(\"The specified path does not exist.\")\n",
    "            continue\n",
    "\n",
    "        nc_files = [f for f in os.listdir(full_path) if f.endswith('.nc')]\n",
    "        file_dates = [parser.parse(f.split('_')[2].split('.')[0]).replace(tzinfo=timezone.utc) for f in nc_files]\n",
    "        \n",
    "        file_nc_data = pd.DataFrame({\n",
    "            'File Name': nc_files,\n",
    "            'Date': file_dates\n",
    "        }).sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "        streamflows = []\n",
    "        for filename, date_nc in file_nc_data.itertuples(index=False):\n",
    "            full_path_file = os.path.join(full_path, filename)\n",
    "            try:\n",
    "                with nc.Dataset(full_path_file, mode='r') as dataset:\n",
    "                    river_ids = dataset.variables['rivid'][:]\n",
    "                    if river_id in river_ids:\n",
    "                        river_index = np.where(river_ids == river_id)[0]\n",
    "                        if river_index.size > 0:\n",
    "                            qout_data = dataset.variables['Qout'][river_index, :][0, 0]\n",
    "                            streamflows.append(qout_data)\n",
    "                    else:\n",
    "                        print(f\"River ID {river_id} not found in {filename}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "        if not streamflows:\n",
    "            continue\n",
    "        \n",
    "        df_for = pd.DataFrame({'dis24_station': streamflows}, index=file_nc_data['Date'])\n",
    "        df_for_daily = df_for.resample('D').mean()\n",
    "        df_for_daily.index += pd.Timedelta(hours=12)\n",
    "\n",
    "        forecast_dict[issue_date] = {\n",
    "            'time': df_for_daily.index,\n",
    "            'dis24_station': df_for_daily['dis24_station'],\n",
    "            'start_date': df_for_daily.index.min()\n",
    "        }\n",
    "\n",
    "    return forecast_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nasa_hymap_MOGREPS(directory_path, file_data, n_s, e_w):\n",
    "    forecast_dict = {}  # Use a dictionary to store the data by issue_date\n",
    "\n",
    "    for forecast in file_data['File Name']:\n",
    "        issue_date = pd.to_datetime(forecast, format='%Y%m%d%H').replace(tzinfo=timezone.utc)\n",
    "        full_path = os.path.join(directory_path, forecast)\n",
    "\n",
    "        if not os.path.exists(full_path):\n",
    "            print(\"The specified path does not exist.\")\n",
    "            continue\n",
    "\n",
    "        nc_files = [f for f in os.listdir(full_path) if f.endswith('.nc')]\n",
    "        file_dates = [parser.parse(f.split('_')[2].split('.')[0]).replace(tzinfo=timezone.utc) for f in nc_files]\n",
    "        \n",
    "        file_nc_data = pd.DataFrame({\n",
    "            'File Name': nc_files,\n",
    "            'Date': file_dates\n",
    "        }).sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "        # Filter rows where the 'Date' column has a time of 12:00 PM\n",
    "        file_nc_data = file_nc_data[file_nc_data['Date'].dt.hour == 12]\n",
    "        file_nc_data = file_nc_data.set_index('Date')\n",
    "\n",
    "\n",
    "        print(file_nc_data)\n",
    "\n",
    "        streamflow_data = []\n",
    "\n",
    "        for filename in file_nc_data['File Name']:\n",
    "            full_path_file = os.path.join(full_path, filename)\n",
    "            try:\n",
    "                with nc.Dataset(full_path_file, mode='r') as dataset:\n",
    "                    # Extract all 18 ensemble values for the specified lat/lon coordinates\n",
    "                    stream = dataset.variables['Streamflow_inst'][:, n_s, e_w]\n",
    "                    streamflow_data.append(list(stream))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "        \n",
    "        # Convert list to NumPy array for matrix operations\n",
    "        streamflow_data_np = np.array(streamflow_data)\n",
    "        print(streamflow_data_np.shape)\n",
    "\n",
    "        dis24_mean = streamflow_data_np.mean(axis=1)\n",
    "\n",
    "        forecast_dict[issue_date] = {\n",
    "            'time': file_nc_data.index,\n",
    "            'dis24_ensem': streamflow_data_np,\n",
    "            'dis24_station': dis24_mean,\n",
    "            'start_date': file_nc_data.index.min()\n",
    "        }\n",
    "\n",
    "    return forecast_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nasa_hymap_GALWEM(directory_path, file_data, n_s, e_w):\n",
    "    forecast_dict = {}  # Use a dictionary to store the data by issue_date\n",
    "\n",
    "    for forecast in file_data['File Name']:\n",
    "        issue_date = pd.to_datetime(forecast, format='%Y%m%d%H').replace(tzinfo=timezone.utc)\n",
    "        full_path = os.path.join(directory_path, forecast)\n",
    "\n",
    "        if not os.path.exists(full_path):\n",
    "            print(\"The specified path does not exist.\")\n",
    "            continue\n",
    "\n",
    "        nc_files = [f for f in os.listdir(full_path) if f.endswith('.nc')]\n",
    "        file_dates = [parser.parse(f.split('_')[2].split('.')[0]).replace(tzinfo=timezone.utc) for f in nc_files]\n",
    "        \n",
    "        file_nc_data = pd.DataFrame({\n",
    "            'File Name': nc_files,\n",
    "            'Date': file_dates\n",
    "        }).sort_values(by='Date').reset_index(drop=True)\n",
    "        \n",
    "        # Filter rows where the 'Date' column has a time of 12:00 PM\n",
    "        file_nc_data = file_nc_data[file_nc_data['Date'].dt.hour == 12]\n",
    "\n",
    "        print(file_nc_data)\n",
    "\n",
    "        file_nc_data = file_nc_data.set_index('Date')\n",
    "\n",
    "        streamflow_data = []\n",
    "\n",
    "        for filename in file_nc_data['File Name']:\n",
    "            full_path_file = os.path.join(full_path, filename)\n",
    "            try:\n",
    "                with nc.Dataset(full_path_file, mode='r') as dataset:\n",
    "                    # Extract all 18 ensemble values for the specified lat/lon coordinates\n",
    "                    stream = dataset.variables['Streamflow_inst'][n_s, e_w]\n",
    "                    streamflow_data.append(stream)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "        \n",
    "        # Convert list to NumPy array for matrix operations\n",
    "        streamflow_data_np = np.array(streamflow_data)\n",
    "        print(streamflow_data_np)\n",
    "\n",
    "\n",
    "        forecast_dict[issue_date] = {\n",
    "            'time': file_nc_data.index,\n",
    "            'dis24_station': streamflow_data_np,\n",
    "            'start_date': file_nc_data.index.min()\n",
    "        }\n",
    "\n",
    "    return forecast_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See files from system\n",
    "directory_path = 'GHI-MR-MOGREPS-G-hymap-ROUTING/'\n",
    "\n",
    "file_data = list_files_and_dates(directory_path)\n",
    "\n",
    "folder_issue = file_data['File Name'].loc[0]\n",
    "full_path = os.path.join(directory_path, folder_issue)\n",
    "\n",
    "if os.path.exists(full_path):\n",
    "    for entry in os.listdir(full_path):\n",
    "        if entry.endswith('.nc'):  # Checking if the file ends with .nc\n",
    "            first_nc_file = entry  # Store the file name\n",
    "            break  # Exit the loop after the first .nc file is found\n",
    "else:\n",
    "    print(\"The specified path does not exist.\")\n",
    "\n",
    "file_path = os.path.join(full_path,first_nc_file)\n",
    "print_netcdf_summary(file_path)\n",
    "\n",
    "\n",
    "# n_s = 640\n",
    "# e_w = 907\n",
    "\n",
    "# # Path to shapefile\n",
    "# shapefile_path = '../Documents/gauge_stations_shapefile.gpkg'\n",
    "\n",
    "# # Calculate longitude and latitude arrays\n",
    "# # Entered manually based on the def print_netcdf_summary output\n",
    "# sw_corner_lon = -179.9296875  # South-West corner longitude\n",
    "# sw_corner_lat = -89.953125    # South-West corner latitude\n",
    "# dx = 0.140625  # Longitude increment\n",
    "# dy = 0.09375   # Latitude increment\n",
    "\n",
    "# # Open the NetCDF file\n",
    "# nc_data = nc.Dataset(file_path, 'r')\n",
    "\n",
    "# # Generate longitude and latitude arrays\n",
    "# longitudes = sw_corner_lon + np.arange(nc_data.dimensions['east_west'].size) * dx\n",
    "# latitudes = sw_corner_lat + np.arange(nc_data.dimensions['north_south'].size) * dy\n",
    "\n",
    "# # Define the latitude and longitude slices for Rio Grande do Sul\n",
    "# lat_bounds = [-32, -26]\n",
    "# lon_bounds = [-57, -49]\n",
    "\n",
    "# # Find the indices for slicing\n",
    "# lat_indices = np.where((latitudes >= lat_bounds[0]) & (latitudes <= lat_bounds[1]))[0]\n",
    "# lon_indices = np.where((longitudes >= lon_bounds[0]) & (longitudes <= lon_bounds[1]))[0]\n",
    "\n",
    "# # Read the streamflow data and slice it\n",
    "# streamflow = nc_data.variables['Streamflow_tavg'][lat_indices, lon_indices]\n",
    "\n",
    "\n",
    "# # Load the shapefile using Geopandas\n",
    "# gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# # Create a plot\n",
    "# fig, ax = plt.subplots(figsize=(12, 10), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "# ax.set_extent([lon_bounds[0], lon_bounds[1], lat_bounds[0], lat_bounds[1]], crs=ccrs.PlateCarree())\n",
    "\n",
    "# # Set colobar limits to better visualization\n",
    "# bar_lim_sup = 2000\n",
    "# bar_lim_inf = 0\n",
    "\n",
    "# # Plot the NetCDF data\n",
    "# cf = ax.pcolormesh(longitudes[lon_indices], latitudes[lat_indices], streamflow, shading='auto', cmap='plasma', vmin=bar_lim_inf, vmax=bar_lim_sup, transform=ccrs.PlateCarree())\n",
    "\n",
    "# # Add the shapefile\n",
    "# gdf.plot(ax=ax, facecolor='black', edgecolor='white', linewidth=1, marker = 'o', transform=ccrs.PlateCarree())\n",
    "\n",
    "# # Adding features\n",
    "# ax.coastlines(resolution='10m', color='black', linewidth=1)\n",
    "# # ax.set_title(f\"Ensemble mean {file_data['Date'].loc[file_n]} - Forecast issued in: {issue_date}\")\n",
    "# plt.colorbar(cf, ax=ax, label='Streamflow (m³/s)')\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "# # Close the NetCDF file\n",
    "# nc_data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # See files from system\n",
    "# directory_path = 'GHI-MR-GALWEM-D-rapid-ROUTING/'\n",
    "\n",
    "# file_data = list_files_and_dates(directory_path)\n",
    "\n",
    "# folder_issue = file_data['File Name'].loc[0]\n",
    "# full_path = os.path.join(directory_path, folder_issue)\n",
    "\n",
    "# if os.path.exists(full_path):\n",
    "#     for entry in os.listdir(full_path):\n",
    "#         if entry.endswith('.nc'):  # Checking if the file ends with .nc\n",
    "#             first_nc_file = entry  # Store the file name\n",
    "#             break  # Exit the loop after the first .nc file is found\n",
    "# else:\n",
    "#     print(\"The specified path does not exist.\")\n",
    "\n",
    "# file_path = os.path.join(full_path,first_nc_file)\n",
    "# print_netcdf_summary(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # See files from system\n",
    "# directory_path = 'GHI-MR-MOGREPS-G-hymap-ROUTING/'\n",
    "\n",
    "# file_data = list_files_and_dates(directory_path)\n",
    "\n",
    "\n",
    "# folder_issue = file_data['File Name'].loc[0]\n",
    "# full_path = os.path.join(directory_path, folder_issue)\n",
    "\n",
    "# if os.path.exists(full_path):\n",
    "#     for entry in os.listdir(full_path):\n",
    "#         if entry.endswith('.nc'):  # Checking if the file ends with .nc\n",
    "#             first_nc_file = entry  # Store the file name\n",
    "#             break  # Exit the loop after the first .nc file is found\n",
    "# else:\n",
    "#     print(\"The specified path does not exist.\")\n",
    "\n",
    "# file_path = os.path.join(full_path,first_nc_file)\n",
    "# print_netcdf_summary(file_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # See files from system\n",
    "# directory_path = 'GHI-MR-MOGREPS-G-rapid-ROUTING/'\n",
    "\n",
    "# file_data = list_files_and_dates(directory_path)\n",
    "\n",
    "\n",
    "# folder_issue = file_data['File Name'].loc[0]\n",
    "# full_path = os.path.join(directory_path, folder_issue)\n",
    "\n",
    "# if os.path.exists(full_path):\n",
    "#     for entry in os.listdir(full_path):\n",
    "#         if entry.endswith('.nc'):  # Checking if the file ends with .nc\n",
    "#             first_nc_file = entry  # Store the file name\n",
    "#             break  # Exit the loop after the first .nc file is found\n",
    "# else:\n",
    "#     print(\"The specified path does not exist.\")\n",
    "\n",
    "# file_path = os.path.join(full_path,first_nc_file)\n",
    "# print_netcdf_summary(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name_data = 'discharge'\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Read data from station\n",
    "info_stations = pd.read_csv('../Documents/porto_alegre_stations_ids.csv')\n",
    "\n",
    "# Specify the directory paths\n",
    "hist_folder_path = f'Historic_{name_data}'\n",
    "telem_folder_path = 'Telemetricas'\n",
    "\n",
    "# Access the first row directly for debugging\n",
    "first_row = info_stations.iloc[0]\n",
    "# Your debugging code goes here using 'first_row' instead of 'row'\n",
    "\n",
    "# Loop through each station using the 'Code' column\n",
    "for index, row in info_stations.iterrows():\n",
    "\n",
    "    print(index)\n",
    "\n",
    "    # if index != 6:\n",
    "    #     continue\n",
    "    \n",
    "    station_name = row['Name']\n",
    "    station_code = row['Code']\n",
    "    station_nasa_code = row['RAPID (rivid)']\n",
    "    n_s = row['LIS_Grid(lat)']\n",
    "    print(n_s)\n",
    "    e_w = row['LIS_Grid(lon)']\n",
    "    print(e_w)\n",
    "    print('----------------------------------------------------------------------------')\n",
    "    print(station_code)\n",
    "    print(station_name)\n",
    "    print(station_nasa_code)\n",
    "\n",
    "\n",
    "\n",
    "    df_telem, bin_data = read_station('../Telemetry', station_code)\n",
    "\n",
    "    if bin_data == 1:\n",
    "        continue\n",
    "\n",
    "    if directory_path == 'GHI-MR-MOGREPS-G-rapid-ROUTING/' or directory_path == 'GHI-MR-GALWEM-D-rapid-ROUTING/':\n",
    "        file_data = list_files_and_dates(directory_path)\n",
    "        processed_data = process_nasa_rapid_MOGREPS(directory_path, file_data, station_nasa_code)\n",
    "\n",
    "# CHECK  BOTH HYMAP \n",
    "\n",
    "    elif directory_path == 'GHI-MR-MOGREPS-G-hymap-ROUTING/':\n",
    "        file_data = list_files_and_dates(directory_path)\n",
    "        processed_data = process_nasa_hymap_MOGREPS(directory_path, file_data, n_s, e_w)\n",
    "\n",
    "    elif directory_path == 'GHI-MR-GALWEM-D-hymap-ROUTING/':\n",
    "        file_data = list_files_and_dates(directory_path)\n",
    "        processed_data = process_nasa_hymap_GALWEM(directory_path, file_data, n_s, e_w)\n",
    "\n",
    "    # Save the processed data to a file\n",
    "    with open(f'{directory_path}/data_nasa_{station_code}.pkl', 'wb') as file:\n",
    "        pickle.dump(processed_data, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
