{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta, datetime, timezone\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import os\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_netcdf_summary(file_path, return_variables=None):\n",
    "\n",
    "    print(return_variables)\n",
    "    \"\"\"Prints a summary and optionally returns data for specific variables from a NetCDF file.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: The file '{file_path}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        dataset = nc.Dataset(file_path, mode='r')\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening file: {e}\")\n",
    "        return\n",
    "\n",
    "    variable_data = {}\n",
    "\n",
    "    print(\"Global Attributes:\")\n",
    "    for attr in dataset.ncattrs():\n",
    "        print(f\"  {attr}: {dataset.getncattr(attr)}\")\n",
    "\n",
    "    print(\"\\nDimensions:\")\n",
    "    for dim, dim_obj in dataset.dimensions.items():\n",
    "        print(f\"  {dim}: length {dim_obj.size} (unlimited: {dim_obj.isunlimited()})\")\n",
    "\n",
    "    print(\"\\nVariables:\")\n",
    "    for var in dataset.variables:\n",
    "        print(f\"Variable: {var}\")\n",
    "        print(f\"  Type: {dataset.variables[var].dtype}\")\n",
    "        print(f\"  Dimensions: {dataset.variables[var].dimensions}\")\n",
    "        print(f\"  Shape: {dataset.variables[var].shape}\")\n",
    "        for attr in dataset.variables[var].ncattrs():\n",
    "            print(f\"    {attr}: {dataset.variables[var].getncattr(attr)}\")\n",
    "        if return_variables and var in return_variables:\n",
    "            variable_data[var] = dataset.variables[var][:]\n",
    "\n",
    "    if 'time' in dataset.variables:\n",
    "        T_var = dataset.variables['time']\n",
    "        T_units = T_var.units if 'units' in T_var.ncattrs() else 'No units available'\n",
    "        T_data = T_var[:]\n",
    "        print(f\"\\nT units: {T_units} Data: {T_data}\")\n",
    "\n",
    "    dataset.close()\n",
    "\n",
    "    if return_variables:\n",
    "        print('retorna!')\n",
    "        return variable_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_station(folder_path, file_name):\n",
    "    \n",
    "    full_path = os.path.join(folder_path, f'{file_name}.txt')\n",
    "    try:\n",
    "        # Load observation data from a text file\n",
    "        # Specify -99999 and -1 as NaN values\n",
    "        df = pd.read_table(full_path, delim_whitespace=True, na_values=[-99999, -1])\n",
    "\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No data: {full_path}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame if file is not found\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def list_files_and_dates(directory_path, file_prefix):\n",
    "    \"\"\"Checks if the directory exists and returns a DataFrame containing filenames and their corresponding dates.\"\"\"\n",
    "    if os.path.exists(directory_path):\n",
    "        all_files = os.listdir(directory_path)\n",
    "        # Filter files that start with the specified prefix\n",
    "        file_list = [file for file in all_files if file.startswith(file_prefix)]\n",
    "        file_dates = [file.split('_')[-1].split('.')[0][:-2] for file in file_list]\n",
    "        \n",
    "        # Convert strings to datetime objects\n",
    "        file_dates = [datetime.strptime(date, '%Y%m%d') for date in file_dates]\n",
    "        \n",
    "        # Create DataFrame with filenames and dates\n",
    "        file_data = pd.DataFrame({\n",
    "            'File Name': file_list,\n",
    "            'Date': file_dates\n",
    "        }).sort_values(by='Date').reset_index(drop=True)\n",
    "        \n",
    "        return file_data\n",
    "    else:\n",
    "        print(f\"The directory '{directory_path}' does not exist.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_hype_data(directory_path, file_data, index_, info_stations):\n",
    "    \n",
    "    forecast_dict = {}  # Use a dictionary to store the data by issue_date\n",
    "\n",
    "    # Iterate over the forecast files\n",
    "    for forecast in range(len(file_data)):\n",
    "        filename = file_data['File Name'].loc[forecast]\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Open the dataset\n",
    "        dataset = nc.Dataset(file_path, mode='r')\n",
    "        \n",
    "        # Extract necessary variables from the dataset\n",
    "        variables = {var: dataset.variables[var][:] for var in dataset.variables}\n",
    "        dis24 = variables['cout']\n",
    "        \n",
    "        id_station = info_stations['WWHOUTID'][index_]\n",
    "        station_index = int(np.where(variables['wwhoutid'] == str(id_station))[0])\n",
    "        \n",
    "        issued_date = file_data['Date'].loc[forecast]\n",
    "        \n",
    "        # Ensure issued_date is aware of timezone\n",
    "        if issued_date.tzinfo is None or issued_date.tzinfo.utcoffset(issued_date) is None:\n",
    "            issued_date = issued_date.replace(tzinfo=timezone.utc)\n",
    "        \n",
    "        start_date = issued_date + timedelta(days=0.5)  # Adjust to 12:00\n",
    "        \n",
    "        # Close the dataset\n",
    "        dataset.close()\n",
    "        \n",
    "        # Extract data for the station\n",
    "        dis24_station = dis24[:, station_index]  # 30 x 51\n",
    "        \n",
    "        # Create a daily time series starting from the date specified in start_date\n",
    "        time = [start_date + timedelta(days=i) for i in range(len(dis24_station))]\n",
    "        \n",
    "        # Store data in the dictionary using the issue_date as the key\n",
    "        forecast_dict[issued_date] = {\n",
    "            'time': time,\n",
    "            'dis24_station': dis24_station,\n",
    "            'start_date': start_date\n",
    "        }\n",
    "\n",
    "    return forecast_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_hype_forecasts(forecast_dict, df_obs, station_name, name_data, index_):\n",
    "    \n",
    "    f = 0\n",
    "    # Iterate over each entry in the dictionary\n",
    "    for issued_date, data in forecast_dict.items():\n",
    "        f = f+1\n",
    "        time = pd.to_datetime(data['time'])  # Convert to datetime\n",
    "\n",
    "        dis24_station = data['dis24_station']\n",
    "        start_date = pd.to_datetime(data['start_date'])\n",
    "\n",
    "        print(f'Issued date: {issued_date}, Start date: {start_date}')\n",
    "        \n",
    "        plt.figure(figsize=(8, 5))\n",
    "        \n",
    "        plt.plot(time, dis24_station, color='black', alpha=0.8, marker = 'o',linestyle = '-', label='Forecast')\n",
    "\n",
    "        # Normalize the observed data 'date' column to midnight\n",
    "        df_obs['date'] = pd.to_datetime(df_obs['date']).dt.normalize()\n",
    "        plt.plot(df_obs['date'], df_obs[name_data], color='blue', label='Observed Data')\n",
    "        \n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Discharge (m³/s)')\n",
    "        plt.title(f'HYPE 24-hour Discharge Forecast for {station_name} Station - Issued on {issued_date}')\n",
    "        plt.xlim(time[0] - timedelta(days=3), time[-1] + timedelta(days=1))\n",
    "        plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\n",
    "        plt.gcf().autofmt_xdate()\n",
    "        plt.grid(True, color='grey', alpha=0.2)\n",
    "        plt.tight_layout()\n",
    "\n",
    "\n",
    "        \n",
    "        plt.axvline(x=issued_date, color='red', linestyle='--', label='Issued Date')\n",
    "        \n",
    "        # Uncomment the following lines if threshold data needs to be plotted\n",
    "        # for key, value in threshold_data.items():\n",
    "        #     plt.axhline(y=value[index_], color='red' if key == 'rl50' else 'orange', linewidth=2, alpha=0.4, label=f'{key.upper()} Threshold')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.savefig(f'Figures/{station_name}_hype_{f}.png', dpi=400)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               File Name       Date\n",
      "0   ww-hype_forecast_odf_20240430R1.0.nc 2024-04-30\n",
      "1   ww-hype_forecast_odf_20240501R1.0.nc 2024-05-01\n",
      "2   ww-hype_forecast_odf_20240502R1.0.nc 2024-05-02\n",
      "3   ww-hype_forecast_odf_20240503R1.0.nc 2024-05-03\n",
      "4   ww-hype_forecast_odf_20240504R1.0.nc 2024-05-04\n",
      "5   ww-hype_forecast_odf_20240505R1.0.nc 2024-05-05\n",
      "6   ww-hype_forecast_odf_20240506R1.0.nc 2024-05-06\n",
      "7   ww-hype_forecast_odf_20240507R1.0.nc 2024-05-07\n",
      "8   ww-hype_forecast_odf_20240508R1.0.nc 2024-05-08\n",
      "9   ww-hype_forecast_odf_20240509R1.0.nc 2024-05-09\n",
      "10  ww-hype_forecast_odf_20240510R1.0.nc 2024-05-10\n",
      "11  ww-hype_forecast_odf_20240511R1.0.nc 2024-05-11\n",
      "12  ww-hype_forecast_odf_20240512R1.0.nc 2024-05-12\n",
      "13  ww-hype_forecast_odf_20240513R1.0.nc 2024-05-13\n",
      "14  ww-hype_forecast_odf_20240514R1.0.nc 2024-05-14\n",
      "15  ww-hype_forecast_odf_20240515R1.0.nc 2024-05-15\n",
      "16  ww-hype_forecast_odf_20240516R1.0.nc 2024-05-16\n",
      "17  ww-hype_forecast_odf_20240517R1.0.nc 2024-05-17\n",
      "18  ww-hype_forecast_odf_20240518R1.0.nc 2024-05-18\n",
      "19  ww-hype_forecast_odf_20240519R1.0.nc 2024-05-19\n",
      "20  ww-hype_forecast_odf_20240520R1.0.nc 2024-05-20\n",
      "21  ww-hype_forecast_odf_20240521R1.0.nc 2024-05-21\n",
      "22  ww-hype_forecast_odf_20240522R1.0.nc 2024-05-22\n",
      "23  ww-hype_forecast_odf_20240523R1.0.nc 2024-05-23\n",
      "24  ww-hype_forecast_odf_20240524R1.0.nc 2024-05-24\n",
      "25  ww-hype_forecast_odf_20240525R1.0.nc 2024-05-25\n",
      "26  ww-hype_forecast_odf_20240526R1.0.nc 2024-05-26\n",
      "27  ww-hype_forecast_odf_20240527R1.0.nc 2024-05-27\n",
      "28  ww-hype_forecast_odf_20240528R1.0.nc 2024-05-28\n",
      "29  ww-hype_forecast_odf_20240529R2.0.nc 2024-05-29\n",
      "30  ww-hype_forecast_odf_20240530R1.0.nc 2024-05-30\n",
      "31  ww-hype_forecast_odf_20240531R1.0.nc 2024-05-31\n",
      "32  ww-hype_forecast_odf_20240601R1.0.nc 2024-06-01\n",
      "33  ww-hype_forecast_odf_20240602R1.0.nc 2024-06-02\n",
      "34  ww-hype_forecast_odf_20240603R1.0.nc 2024-06-03\n",
      "35  ww-hype_forecast_odf_20240604R1.0.nc 2024-06-04\n",
      "36  ww-hype_forecast_odf_20240605R1.0.nc 2024-06-05\n",
      "37  ww-hype_forecast_odf_20240606R1.0.nc 2024-06-06\n",
      "38  ww-hype_forecast_odf_20240607R1.0.nc 2024-06-07\n",
      "39  ww-hype_forecast_odf_20240608R1.0.nc 2024-06-08\n",
      "40  ww-hype_forecast_odf_20240609R1.0.nc 2024-06-09\n",
      "41  ww-hype_forecast_odf_20240610R1.0.nc 2024-06-10\n"
     ]
    }
   ],
   "source": [
    "# See files from system\n",
    "directory_path = 'brazil_delivery/'\n",
    "prefix = \"ww-hype_forecast_odf_\"\n",
    "file_data = list_files_and_dates(directory_path, prefix)\n",
    "\n",
    "if file_data is not None:\n",
    "    print(file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Global Attributes:\n",
      "  title: ww-hype 1.3.9 10d forecast\n",
      "  institution: SMHI\n",
      "  created: 2024-04-30 12:05\n",
      "  format_version: 1\n",
      "  coordinates: wwhoutid\n",
      "\n",
      "Dimensions:\n",
      "  time: length 10 (unlimited: False)\n",
      "  basin: length 14 (unlimited: False)\n",
      "\n",
      "Variables:\n",
      "Variable: cout\n",
      "  Type: float32\n",
      "  Dimensions: ('time', 'basin')\n",
      "  Shape: (10, 14)\n",
      "    _FillValue: nan\n",
      "    title: simulated outflow from olake/subbasin, only positive flow (outflow)\n",
      "    units: m³/s\n",
      "    coordinates: subid\n",
      "Variable: time\n",
      "  Type: int64\n",
      "  Dimensions: ('time',)\n",
      "  Shape: (10,)\n",
      "    units: seconds since 1970-01-01\n",
      "    calendar: proleptic_gregorian\n",
      "Variable: wwhoutid\n",
      "  Type: <class 'str'>\n",
      "  Dimensions: ('basin',)\n",
      "  Shape: (14,)\n",
      "\n",
      "T units: seconds since 1970-01-01 Data: [1714435200 1714521600 1714608000 1714694400 1714780800 1714867200\n",
      " 1714953600 1715040000 1715126400 1715212800]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filename = file_data['File Name'].loc[0]\n",
    "file_path = os.path.join(directory_path, filename)\n",
    "print_netcdf_summary(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "87450004\n",
      "CAIS MAUÁ C6\n",
      "----------------------------------------------------------------------------\n",
      "87010000\n",
      "TRIUNFO\n",
      "----------------------------------------------------------------------------\n",
      "87270000\n",
      "PASSO MONTENEGRO\n",
      "----------------------------------------------------------------------------\n",
      "87382000\n",
      "SÃO LEOPOLDO\n",
      "----------------------------------------------------------------------------\n",
      "86950000\n",
      "TAQUARI\n",
      "----------------------------------------------------------------------------\n",
      "87401750\n",
      "CORSAN ALVORADA\n",
      "----------------------------------------------------------------------------\n",
      "85900000\n",
      "RIO PARDO\n",
      "----------------------------------------------------------------------------\n",
      "87160000\n",
      "NOVA PALMIRA\n",
      "----------------------------------------------------------------------------\n",
      "87380000\n",
      "CAMPO BOM\n",
      "----------------------------------------------------------------------------\n",
      "87399000\n",
      "PASSO DAS CANOAS - AUXILIAR\n",
      "----------------------------------------------------------------------------\n",
      "86895000\n",
      "PORTO MARIANTE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "name_data = 'discharge'\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Read data from station\n",
    "info_stations = pd.read_csv('porto_alegre_stations_wwhoutid.csv')\n",
    "\n",
    "# Specify the directory paths\n",
    "hist_folder_path = f'Historic_{name_data}'\n",
    "telem_folder_path = 'Telemetricas'\n",
    "\n",
    "# Loop through each station using the 'Code' column\n",
    "for index, row in info_stations.iterrows():\n",
    "    station_name = row['Name']\n",
    "    station_code = row['Code']\n",
    "    print('----------------------------------------------------------------------------')\n",
    "    print(station_code)\n",
    "    print(station_name)\n",
    "\n",
    "    # Read telemetric data\n",
    "    df_telem = read_station('../Telemetricas', station_code)\n",
    "\n",
    "    # Process data\n",
    "    processed_data = process_hype_data(directory_path, file_data, index, info_stations)\n",
    "\n",
    "\n",
    "    # Save the processed data to a file\n",
    "    with open(f'data_hype_{station_code}.pkl', 'wb') as file:\n",
    "        pickle.dump(processed_data, file)\n",
    "\n",
    "    # Plot the forecasts\n",
    "    # plot_hype_forecasts(processed_data, df_telem, station_name, name_data, index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
